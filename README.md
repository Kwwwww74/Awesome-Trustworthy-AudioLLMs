# ğŸ§ Awesome Trustworthy Audio-LLMs  
<p align="center">
  <img src="./images/logo.jpg" alt="Awesome Trustworthy Audio-LLMs" width="80%">
</p>

*A curated and continuously evolving collection of research, benchmarks, datasets, and open resources on **Trustworthy Audio Large Language Models (Audio-LLMs)** â€” spanning the entire spectrum of **ALLM Safety**. In addition, there are also related papers about audio large models.*

---

## ğŸŒIntroduction  

With the rapid developmenst of **Audio Large Language Models (Audio-LLMs)**, ensuring their **trustworthiness and safety** has become an essential research frontier.  

This repository(TALLM) serves as a **comprehensive and community-driven hub** for tracking progress in the field of **trustworthy audio intelligence**. 

It highlights research on:
- ğŸ›¡ï¸ **Safety**
- âš–ï¸ **Fairness**  
- ğŸ”® **Hallucination** 
- ğŸ” **Privacy** 
- âš™ï¸ **Robustness**
- ğŸ”– **Authentication**
- ğŸ’¡ **Interpretability**
- ğŸ’» **Security**
- ğŸ” **Watermark**

Together, these works aim toward a future where Audio-LLMs are **not only capable of understanding voices â€” but also worthy of being trusted**.

---

## ğŸ§­ Research Collections 
  
- Paper
   - A. Safety
      - [A0. General](./papers/safety/general.md)
      - [A1. Jailbreak](./papers/safety/jailbreak.md)
      - [A2. Alignment](./papers/safety/alignment.md)
      - [A3. Deepfake](./papers/safety/deepfake.md)
      - [A4. Ethics](./papers/safety/ethics.md)
      - [A5. Fairness](./papers/safety/fairness.md)
      - [A6. Hallucination](./papers/safety/hallucination.md)
      - [A7. Prompt Injection](./papers/safety/prompt_injection.md)
      - [A8. Toxicity](./papers/safety/toxicity.md)
      - [A9. Interpretability](./papers/safety/interpretability.md)
  - B. Security
      - [B0. General](./papers/security/general.md)
      - [B1. Adversarial Examples](./papers/security/adversarial_examples.md)
      - [B2. Agent](./papers/security/agent.md)
      - [B3. Poison & Backdoor](./papers/security/poison_and_backdoor.md)
      - [B4. Side-Channel](./papers/security/side_channel.md)
      - [B5. System](./papers/security/system.md)
      - [B6. Interpretability](./papers/security/interpretability.md)
  - C. Privacy
      - [C0. General](./papers/privacy/general.md)
      - [C1. Contamination](./papers/privacy/contamination.md)
      - [C2. Data Reconstruction](./papers/privacy/data_reconstruction.md)
      - [C3. Membership Inference Attacks](./papers/privacy/membership_inference_attacks.md)
      - [C4. Model Extraction](./papers/privacy/model_extraction.md)
      - [C5. Privacy-Preserving Computation](./papers/privacy/privacy_preserving_computation.md)
      - [C6. Property Inference Attacks](./papers/privacy/property_inference_attacks.md)
      - [C7. Side-Channel](./papers/privacy/side_channel.md)
      - [C8. Unlearning](./papers/privacy/unlearning.md)
      - [C9. Watermark & Copyright](./papers/privacy/watermark_and_copyright.md)

### In addition to the above-mentioned ones:
If you want to learn more about Audio Large Language Models, you can take a look at the following.
  - ğŸš€ [Survey](collections/survey.md)
  - ğŸš€ [Technical Report for Audio Large Language Models](collections/technical_report.md)

---

## ğŸ—ï¸ Recent News  
- **[2025.11.12]** ğŸ£TALLM is released!!! 

---

## ğŸ¤ How to Contribute  

*We welcome contributions from researchers and practitioners!  If you'd like to submit a piece of writing, please write an email to **kaiwenluo74@gmail.com**. Then we can add the article in.*

---

## Acknowledgement

- Organizers: [Kevin Luo (ç½—å‡¯æ–‡)](https://scholar.google.com/citations?user=XnZXByMAAAAJ&hl=en), [Liang Lin (æ—äº®)](https://scholar.google.com/citations?user=XQNpChIAAAAJ&hl=en), [Yibo Zhang (å¼ å¥•åš)], [Zhenhong Zhou(å‘¨æŒ¯å®)](https://ydyjya.github.io)

<p align="center">
  <img src="./images/school.jpg" alt="Awesome Trustworthy Audio-LLMs" width="80%">
</p>
