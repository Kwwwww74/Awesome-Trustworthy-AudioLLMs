# ğŸ§ Awesome Trustworthy Audio-LLMs  
<p align="center">
  <img src="./images/logo.jpg" alt="Awesome Trustworthy Audio-LLMs" width="80%">
</p>

*A curated and continuously evolving collection of research, benchmarks, datasets, and open resources on **Trustworthy Audio Large Language Models (Audio-LLMs)** â€” spanning the entire spectrum of **ALLM Safety**.*

---

## ğŸŒIntroduction  

With the rapid developmenst of **Audio Large Language Models (Audio-LLMs)**, ensuring their **trustworthiness and safety** has become an essential research frontier.  

This repository(TALLM) serves as a **comprehensive and community-driven hub** for tracking progress in the field of **trustworthy audio intelligence**. 

It highlights research on:
- ğŸ›¡ï¸ **Safety**
- âš–ï¸ **Fairness**  
- ğŸ”® **Hallucination** 
- ğŸ” **Privacy** 
- âš™ï¸ **Robustness**
- ğŸ”– **Authentication**

Together, these works aim toward a future where Audio-LLMs are **not only capable of understanding voices â€” but also worthy of being trusted**.

---

## ğŸ§­ Research Collections  

- [Safety](./papers/safety)
  - [Surveys & Overviews](./papers/safety/surveys--overviews)
  - [Attacks & Risks](./papers/safety/attacks--risks)
  - [Defense & Safety Alignment](./papers/safety/defense--safety-alignment)
  - [Benchmarks & Evaluation](./papers/safety/benchmarks--evaluation)
- [Fairness](./papers/fairness)
  - [Surveys & Overviews](./papers/fairness/surveys--overviews)
  - [Attacks & Risks](./papers/fairness/attacks--risks)
  - [Defense & Safety Alignment](./papers/fairness/defense--safety-alignment)
  - [Benchmarks & Evaluation](./papers/fairness/benchmarks--evaluation)
- [Hallucination](./papers/hallucination)
  - [Surveys & Overviews](./papers/hallucination/surveys--overviews)
  - [Attacks & Risks](./papers/hallucination/attacks--risks)
  - [Defense & Safety Alignment](./papers/hallucination/defense--safety-alignment)
  - [Benchmarks & Evaluation](./papers/hallucination/benchmarks--evaluation)
- [Privacy](./papers/privacy)
  - [Surveys & Overviews](./papers/privacy/surveys--overviews)
  - [Attacks & Risks](./papers/privacy/attacks--risks)
  - [Defense & Safety Alignment](./papers/privacy/defense--safety-alignment)
  - [Benchmarks & Evaluation](./papers/privacy/benchmarks--evaluation)
- [Robustness](./papers/robustness)
  - [Surveys & Overviews](./papers/robustness/surveys--overviews)
  - [Attacks & Risks](./papers/robustness/attacks--risks)
  - [Defense & Safety Alignment](./papers/robustness/defense--safety-alignment)
  - [Benchmarks & Evaluation](./papers/robustness/benchmarks--evaluation)
- [Authentication](./papers/authentication)
  - [Surveys & Overviews](./papers/authentication/surveys--overviews)
  - [Attacks & Risks](./papers/authentication/attacks--risks)
  - [Defense & Safety Alignment](./papers/authentication/defense--safety-alignment)
  - [Benchmarks & Evaluation](./papers/authentication/benchmarks--evaluation)

---

## ğŸ—ï¸ Recent News  
- **[2025.11.12]** ğŸ£TALLM is released!!! 

---

## ğŸ¤ How to Contribute  

We welcome contributions from researchers and practitioners!  

---

## Acknowledgement

- Organizers: [Kevin Luo (ç½—å‡¯æ–‡)](https://tianshuocong.github.io/), [Xinlei He (ä½•æ–°ç£Š)](https://xinleihe.github.io/), [Zhengyu Zhao (èµµæ­£å®‡)](https://zhengyuzhao.github.io/), [Yugeng Liu (åˆ˜ç¦¹æ›´)](https://liu.ai/), [Delong Ran (å†‰å¾·é¾™)](https://github.com/eggry)

- This project is inspired by [LLM Security](https://llmsecurity.net/), [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security), [LLM Security & Privacy](https://github.com/chawins/llm-sp),             [UR2-LLMs](https://github.com/jxzhangjhu/Awesome-LLM-Uncertainty-Reliability-Robustness), [PLMpapers](https://github.com/thunlp/PLMpapers), [EvaluationPapers4ChatGPT](https://github.com/THU-KEG/EvaluationPapers4ChatGPT)

<p align="center"><img src="images/school.png" width="900" /></p>
